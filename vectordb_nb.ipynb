{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9717c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c777795",
   "metadata": {},
   "source": [
    "## Project 1: A Hybrid Vector Database with Reranking and Filtering\n",
    "\n",
    "This project is a great way to explore the full lifecycle of a RAG pipeline's retrieval component. A hybrid approach combines the speed of vector search with the precision of traditional keyword search and reranking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338f600",
   "metadata": {},
   "source": [
    "### Core Components:\n",
    "\n",
    "* Vector Indexing: Build an in-memory or on-disk vector index using a library like FAISS or ScaNN. Your database should be able to add, delete, and update vectors.\n",
    "\n",
    "* Metadata Storage: A relational database (like SQLite or PostgreSQL) or a simple JSON file store to hold the original text chunks and associated metadata (e.g., document ID, author, date, categories).\n",
    "\n",
    "* Hybrid Search:\n",
    "\n",
    "    * Vector Search: Use your vector index to find the top N most semantically similar documents to a user's query.\n",
    "\n",
    "    * Keyword Search: Implement a basic keyword search using a library like whoosh or even a simple inverted index to find documents with exact keyword matches.\n",
    "\n",
    "    * Combining Results: Fuse the results from both searches. You can use a simple scoring system that weighs both semantic similarity and keyword presence.\n",
    "\n",
    "* Reranking: The top results from the hybrid search may not be in the best order. Implement a reranker model (a small, performant cross-encoder from Hugging Face) that takes the user query and each of the top-retrieved documents and re-scores them to improve the final ranking.\n",
    "\n",
    "* Advanced Filtering: Allow users to filter their search results based on the metadata. For example, a user could search for \"deep learning\" but filter the results to only include documents from the \"2024\" year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb7f63",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "The goal of this project is to build a complete, end-to-end retrieval system that goes beyond a simple vector database. The system will be a \"hybrid\" search engine that combines three key retrieval methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cc417",
   "metadata": {},
   "source": [
    "* Vector Search: For semantic understanding and finding documents conceptually similar to a query. Documents and queries are converted into numerical vectors (embeddings). The system finds documents whose vectors are geometrically closest to the query vector.\n",
    "\n",
    "* Keyword Search: For precise matching of specific terms, which can be critical for proper names, acronyms, and direct references. A traditional search technique using an inverted index. It's excellent for finding exact term matches and is highly performant.\n",
    "\n",
    "* Reranking: To re-order the top-K results from the hybrid search, ensuring the most relevant documents are at the very top. A secondary, more powerful model (typically a cross-encoder) is used to re-evaluate the top results from the initial hybrid search. This is computationally more expensive, but since it only operates on a small number of candidates, it significantly improves the final accuracy.\n",
    "\n",
    "* Hybrid Search: The process of combining the results from both vector and keyword searches. The challenge here is how to score and merge the results to get a single, ranked list.\n",
    "\n",
    "* Metadata Filtering: The ability to narrow down search results based on specific attributes of the documents, such as author, date, or category. This adds a powerful layer of user control to the search process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f3cc3",
   "metadata": {},
   "source": [
    "### Proposed System Architecture\n",
    "\n",
    "The system will have two primary phases: an Ingestion Pipeline and a Query Pipeline.\n",
    "\n",
    "1. Ingestion Pipeline: This is where you prepare your knowledge base.\n",
    "\n",
    "Document Loader: A component that reads in raw documents (e.g., text files, PDFs).\n",
    "\n",
    "Text Splitter: Breaks down large documents into smaller, manageable chunks. This is crucial for RAG, as you want to retrieve specific relevant snippets, not entire long documents.\n",
    "\n",
    "Embedding Model: Converts each text chunk into a vector embedding.\n",
    "\n",
    "Metadata Extractor: Parses and extracts metadata (e.g., title, year, author, source) for each chunk.\n",
    "\n",
    "Storage: Stores the text chunks, their embeddings, and the associated metadata.\n",
    "\n",
    "2. Query Pipeline: This is how the system responds to a user's query.\n",
    "\n",
    "User Query: A user inputs a query and any desired filters.\n",
    "\n",
    "Query Embedding: The user's query is converted into an embedding using the same model as the ingestion pipeline.\n",
    "\n",
    "Parallel Search: The query is sent to two different search components simultaneously.\n",
    "\n",
    "Vector Database: Performs a semantic search to find the top-K semantically similar documents.\n",
    "\n",
    "Keyword Search Index: Performs a keyword search to find the top-K documents with matching terms.\n",
    "\n",
    "Hybrid Combination: The results from both searches are combined and a score is calculated for each document. A simple method is to re-rank the union of the two result sets.\n",
    "\n",
    "Metadata Filtering: The combined results are filtered to only include documents that match the user's specified metadata criteria.\n",
    "\n",
    "Reranking Model: The final top-N documents from the hybrid search are passed to a reranker model, which re-sorts them for optimal relevance.\n",
    "\n",
    "Final Results: The top documents are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f3e4",
   "metadata": {},
   "source": [
    "## Suggested Technology Stack\n",
    "\n",
    "Programming Language: Python is the standard for this type of project due to its rich ecosystem of libraries.\n",
    "\n",
    "Embedding Model: Use a pre-trained model from Hugging Face via the sentence-transformers library.\n",
    "\n",
    "Recommendation: BAAI/bge-small-en-v1.5 for strong performance.\n",
    "\n",
    "Vector Index: The FAISS library from Facebook AI is a highly optimized and performant library for similarity search.\n",
    "\n",
    "Keyword Index: The Whoosh library is a pure-Python library for creating and searching text. It's a great choice for this project as it's simple to set up and use.\n",
    "\n",
    "Reranking Model: Use a cross-encoder model from Hugging Face, also via the sentence-transformers library.\n",
    "\n",
    "Recommendation: cross-encoder/ms-marco-MiniLM-L-6-v2 is a good, lightweight choice.\n",
    "\n",
    "Data Storage: A simple JSON file or a lightweight database like TinyDB can be used to store the original text chunks and their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10b44e",
   "metadata": {},
   "source": [
    "## Project Workflow and Code Structure\n",
    "\n",
    "Your project can be organized into a few key modules:\n",
    "\n",
    "ingestion.py:\n",
    "\n",
    "A function to load a document.\n",
    "\n",
    "A function to split text into chunks.\n",
    "\n",
    "A function to generate embeddings using sentence-transformers.\n",
    "\n",
    "A function to build the FAISS index and the Whoosh index, and save the metadata to a file.\n",
    "\n",
    "database.py:\n",
    "\n",
    "A class that acts as the main interface to your hybrid database.\n",
    "\n",
    "Methods like add_documents(), delete_documents(), and hybrid_search().\n",
    "\n",
    "It will handle loading the FAISS index, the Whoosh index, and the metadata on startup.\n",
    "\n",
    "query.py:\n",
    "\n",
    "A simple command-line interface or a main function that demonstrates the query process.\n",
    "\n",
    "Takes user input for the query and filters.\n",
    "\n",
    "Calls the hybrid_search() method from your database class and prints the final reranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running, you will need to install the required libraries:\n",
    "# pip install sentence-transformers faiss-cpu whoosh\n",
    "# We will use faiss-cpu for simplicity, but for GPU support, you would install faiss-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da6a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (5.0.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (4.53.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amith\\anaconda3\\envs\\thellmbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.9 MB 349.5 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 0.5/14.9 MB 349.5 kB/s eta 0:00:42\n",
      "   -- ------------------------------------- 0.8/14.9 MB 409.3 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 0.8/14.9 MB 409.3 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 1.0/14.9 MB 470.4 kB/s eta 0:00:30\n",
      "   -- ------------------------------------- 1.0/14.9 MB 470.4 kB/s eta 0:00:30\n",
      "   --- ------------------------------------ 1.3/14.9 MB 520.1 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 1.3/14.9 MB 520.1 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 537.8 kB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 578.5 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 2.1/14.9 MB 611.7 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 2.4/14.9 MB 651.5 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 2.4/14.9 MB 651.5 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 2.6/14.9 MB 680.1 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.1/14.9 MB 747.2 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.4/14.9 MB 777.3 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 3.7/14.9 MB 810.8 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 3.9/14.9 MB 835.8 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 4.5/14.9 MB 897.8 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 4.7/14.9 MB 923.0 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 5.2/14.9 MB 971.9 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 5.5/14.9 MB 995.6 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.8/14.9 MB 1.0 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 6.3/14.9 MB 1.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 6.6/14.9 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 6.8/14.9 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 7.3/14.9 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 7.6/14.9 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 8.1/14.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 8.7/14.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 9.2/14.9 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 9.7/14.9 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 10.2/14.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 10.5/14.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.0/14.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.8/14.9 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.1/14.9 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.6/14.9 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.1/14.9 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.6/14.9 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.9 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Installing collected packages: whoosh, faiss-cpu\n",
      "\n",
      "   ---------------------------------------- 0/2 [whoosh]\n",
      "   ---------------------------------------- 0/2 [whoosh]\n",
      "   ---------------------------------------- 0/2 [whoosh]\n",
      "   ---------------------------------------- 0/2 [whoosh]\n",
      "   -------------------- ------------------- 1/2 [faiss-cpu]\n",
      "   -------------------- ------------------- 1/2 [faiss-cpu]\n",
      "   -------------------- ------------------- 1/2 [faiss-cpu]\n",
      "   -------------------- ------------------- 1/2 [faiss-cpu]\n",
      "   ---------------------------------------- 2/2 [faiss-cpu]\n",
      "\n",
      "Successfully installed faiss-cpu-1.11.0.post1 whoosh-2.7.4\n"
     ]
    }
   ],
   "source": [
    "# ! pip install sentence-transformers faiss-cpu whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121fedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "592333ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document ingestion pipeline...\n",
      "Building FAISS vector index...\n",
      "FAISS index built and saved.\n",
      "Building Whoosh keyword index...\n",
      "Whoosh index built and saved.\n",
      "Saving metadata...\n",
      "Metadata saved.\n",
      "Ingestion pipeline complete.\n",
      "Initializing HybridDatabase...\n",
      "Database initialized and ready to query.\n",
      "--- Example 1: Semantic Query ---\n",
      "\n",
      "Searching for: 'How do LLMs work?'...\n",
      "1. The core technology of LLMs is based on deep learning principles..\n",
      "2. Large Language Models, or LLMs, are at the forefront of this revolution.\n",
      "3. Transfer learning is a technique that uses pre-trained models.\n",
      "\n",
      "--- Example 2: Keyword Query ---\n",
      "\n",
      "Searching for: 'vector dbt'...\n",
      "1. Vector databases are specialized databases for storing and searching vector embeddings.\n",
      "2. FAISS and Milvus are popular examples of vector database technologies.\n",
      "\n",
      "--- Example 3: Hybrid Query with Metadata Filter ---\n",
      "\n",
      "Searching for: 'what is transfer learning in AI?'...\n",
      "1. Transfer learning is a technique that uses pre-trained models.\n",
      "2. Deep learning is a subset of machine learning that utilizes neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Before running, you will need to install the required libraries:\n",
    "# pip install sentence-transformers faiss-cpu whoosh\n",
    "# We will use faiss-cpu for simplicity, but for GPU support, you would install faiss-gpu.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from whoosh.filedb.filestore import FileStorage\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.qparser.default import MultifieldParser\n",
    "\n",
    "class HybridDatabase:\n",
    "    \"\"\"\n",
    "    A class to manage the hybrid vector database, including loading indices,\n",
    "    performing searches, and reranking results.\n",
    "    \"\"\"\n",
    "    def __init__(self, index_dir=\"index\"):\n",
    "        \"\"\"\n",
    "        Initializes the database by loading the pre-built FAISS index, Whoosh index,\n",
    "        and metadata. Also loads the embedding and reranking models.\n",
    "        \"\"\"\n",
    "        print(\"Initializing HybridDatabase...\")\n",
    "        self.index_dir = index_dir\n",
    "        self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "        self.reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        self.faiss_index = None\n",
    "        self.whoosh_index = None\n",
    "        self.metadata = []\n",
    "\n",
    "        # Load the indices and metadata from the specified directory\n",
    "        self._load_indices()\n",
    "        print(\"Database initialized and ready to query.\")\n",
    "\n",
    "    def _load_indices(self):\n",
    "        \"\"\"\n",
    "        Loads the FAISS index, Whoosh index, and metadata from the index directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load FAISS index\n",
    "            self.faiss_index = faiss.read_index(os.path.join(self.index_dir, \"faiss.index\"))\n",
    "\n",
    "            # Load Whoosh index\n",
    "            storage = FileStorage(os.path.join(self.index_dir, \"whoosh\"))\n",
    "            self.whoosh_index = storage.open_index()\n",
    "\n",
    "            # Load metadata\n",
    "            with open(os.path.join(self.index_dir, \"metadata.json\"), 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading indices: {e}\")\n",
    "            print(\"Please run the ingestion pipeline first.\")\n",
    "            exit()\n",
    "\n",
    "    def hybrid_search(self, query: str, top_k: int = 10, alpha: float = 0.5, filters: dict = None):\n",
    "        \"\"\"\n",
    "        Performs a hybrid search combining vector search and keyword search.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's search query.\n",
    "            top_k (int): The number of top results to return.\n",
    "            alpha (float): A weighting factor for combining vector and keyword scores.\n",
    "            filters (dict): A dictionary of metadata filters (e.g., {\"year\": 2023}).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of reranked and filtered document snippets.\n",
    "        \"\"\"\n",
    "        # 1. Perform parallel searches\n",
    "        print(f\"\\nSearching for: '{query}'...\")\n",
    "\n",
    "        # Vector search (semantic search)\n",
    "        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        vector_distances, vector_ids = self.faiss_index.search(query_embedding, top_k * 2)\n",
    "\n",
    "        # Keyword search\n",
    "        keyword_results = []\n",
    "        with self.whoosh_index.searcher() as searcher:\n",
    "            query_parser = MultifieldParser([\"content\", \"title\"], schema=self.whoosh_index.schema)\n",
    "            parsed_query = query_parser.parse(query)\n",
    "            results = searcher.search(parsed_query, limit=top_k * 2)\n",
    "            # The fix: cast the doc_id from Whoosh (a string) to an integer\n",
    "            keyword_results = [int(result['doc_id']) for result in results]\n",
    "\n",
    "        # 2. Combine and filter results\n",
    "        # Collect a union of unique document IDs from both search methods\n",
    "        unique_doc_ids = set(vector_ids[0]).union(set(keyword_results))\n",
    "        combined_candidates = []\n",
    "        for doc_id in unique_doc_ids:\n",
    "            # This line will now work correctly because all doc_ids are integers\n",
    "            doc_metadata = self.metadata[doc_id]\n",
    "            \n",
    "            # Apply metadata filters\n",
    "            if filters:\n",
    "                is_filtered = False\n",
    "                for key, value in filters.items():\n",
    "                    if key in doc_metadata and doc_metadata[key] != value:\n",
    "                        is_filtered = True\n",
    "                        break\n",
    "                if is_filtered:\n",
    "                    continue\n",
    "\n",
    "            combined_candidates.append(doc_metadata['content'])\n",
    "        \n",
    "        # If no results after filtering, return empty list\n",
    "        if not combined_candidates:\n",
    "            print(\"No results found after filtering.\")\n",
    "            return []\n",
    "\n",
    "        # 3. Rerank the top candidates\n",
    "        # Create a list of tuples for the reranker model: [(query, doc1), (query, doc2), ...]\n",
    "        cross_encoder_input = [(query, doc) for doc in combined_candidates]\n",
    "        reranker_scores = self.reranker_model.predict(cross_encoder_input)\n",
    "\n",
    "        # 4. Sort the candidates based on reranker scores\n",
    "        scored_results = sorted(zip(combined_candidates, reranker_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top_k results\n",
    "        return [result[0] for result in scored_results[:top_k]]\n",
    "\n",
    "\n",
    "def ingest_documents(documents: list, index_dir=\"index\"):\n",
    "    \"\"\"\n",
    "    Ingests documents into the hybrid database. This function simulates\n",
    "    the ingestion pipeline, creating and saving the indices and metadata.\n",
    "    \"\"\"\n",
    "    print(\"Starting document ingestion pipeline...\")\n",
    "    # Create index directories\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "    if not os.path.exists(os.path.join(index_dir, \"whoosh\")):\n",
    "        os.makedirs(os.path.join(index_dir, \"whoosh\"))\n",
    "\n",
    "    # Load the embedding model\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "    # Prepare document chunks and metadata\n",
    "    metadata = []\n",
    "    chunk_id = 0\n",
    "    # For a real-world app, you would use a proper text splitter here.\n",
    "    # We will use simple splitting for this example.\n",
    "    for doc in documents:\n",
    "        # Split documents into smaller chunks\n",
    "        chunks = doc['content'].split('. ')\n",
    "        for chunk in chunks:\n",
    "            if chunk:\n",
    "                # Add a unique ID and other metadata\n",
    "                chunk_metadata = {\n",
    "                    \"doc_id\": chunk_id,\n",
    "                    \"title\": doc.get('title', 'Unknown'),\n",
    "                    \"author\": doc.get('author', 'Unknown'),\n",
    "                    \"year\": doc.get('year', 'Unknown'),\n",
    "                    \"content\": chunk.strip() + '.'\n",
    "                }\n",
    "                metadata.append(chunk_metadata)\n",
    "                chunk_id += 1\n",
    "\n",
    "    # 1. Build FAISS index for vector search\n",
    "    print(\"Building FAISS vector index...\")\n",
    "    text_chunks = [item['content'] for item in metadata]\n",
    "    embeddings = embedding_model.encode(text_chunks, convert_to_numpy=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    faiss_index.add(embeddings)\n",
    "    faiss.write_index(faiss_index, os.path.join(index_dir, \"faiss.index\"))\n",
    "    print(\"FAISS index built and saved.\")\n",
    "\n",
    "    # 2. Build Whoosh index for keyword search\n",
    "    print(\"Building Whoosh keyword index...\")\n",
    "    schema = Schema(doc_id=ID(stored=True), title=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "                    author=TEXT(stored=True), year=NUMERIC(stored=True),\n",
    "                    content=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
    "    whoosh_path = os.path.join(index_dir, \"whoosh\")\n",
    "    whoosh_ix = create_in(whoosh_path, schema)\n",
    "    writer = whoosh_ix.writer()\n",
    "    for item in metadata:\n",
    "        writer.add_document(doc_id=str(item['doc_id']), title=item['title'], author=item['author'],\n",
    "                            year=item['year'], content=item['content'])\n",
    "    writer.commit()\n",
    "    print(\"Whoosh index built and saved.\")\n",
    "\n",
    "    # 3. Save metadata\n",
    "    print(\"Saving metadata...\")\n",
    "    with open(os.path.join(index_dir, \"metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(\"Metadata saved.\")\n",
    "    print(\"Ingestion pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Mock data to simulate new documents being added to the knowledge base\n",
    "    mock_documents = [\n",
    "        {\n",
    "            \"title\": \"The Rise of AI\",\n",
    "            \"author\": \"Alice\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Artificial intelligence has seen a massive surge in popularity. \"\n",
    "                \"Large Language Models, or LLMs, are at the forefront of this revolution. \"\n",
    "                \"These models use a transformer architecture to process and generate human-like text. \"\n",
    "                \"The core technology of LLMs is based on deep learning principles.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Deep Learning Techniques\",\n",
    "            \"author\": \"Bob\",\n",
    "            \"year\": 2023,\n",
    "            \"content\": (\n",
    "                \"Deep learning is a subset of machine learning that utilizes neural networks. \"\n",
    "                \"A key component is the backpropagation algorithm. \"\n",
    "                \"Transfer learning is a technique that uses pre-trained models. \"\n",
    "                \"Researchers are exploring new ways to optimize these complex models.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Vector Databases Explained\",\n",
    "            \"author\": \"Alice\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Vector databases are specialized databases for storing and searching vector embeddings. \"\n",
    "                \"They are crucial for efficient semantic search in RAG applications. \"\n",
    "                \"FAISS and Milvus are popular examples of vector database technologies. \"\n",
    "                \"These databases use approximate nearest neighbor algorithms.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Quantum Computing\",\n",
    "            \"author\": \"Charlie\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Quantum computing promises to solve problems intractable for classical computers. \"\n",
    "                \"It uses principles of quantum mechanics, such as superposition and entanglement. \"\n",
    "                \"Qubits are the fundamental units of quantum information. \"\n",
    "                \"The field is still in its early stages of development.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Step 1: Ingestion ---\n",
    "    # This step would typically be run periodically to update the knowledge base.\n",
    "    # It creates all the necessary indices and stores the metadata.\n",
    "    ingest_documents(mock_documents)\n",
    "\n",
    "    # --- Step 2: Querying ---\n",
    "    # This simulates a user querying the already-ingested knowledge base.\n",
    "    db = HybridDatabase()\n",
    "\n",
    "    # Example 1: Pure semantic search query\n",
    "    print(\"--- Example 1: Semantic Query ---\")\n",
    "    results = db.hybrid_search(query=\"How do LLMs work?\", top_k=3)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n",
    "\n",
    "    # Example 2: Keyword-heavy search query\n",
    "    print(\"\\n--- Example 2: Keyword Query ---\")\n",
    "    results = db.hybrid_search(query=\"vector dbt\", top_k=2)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n",
    "\n",
    "    # Example 3: Hybrid query with filtering\n",
    "    print(\"\\n--- Example 3: Hybrid Query with Metadata Filter ---\")\n",
    "    # This query looks for information about AI, but only from documents published in 2023\n",
    "    results = db.hybrid_search(query=\"what is transfer learning in AI?\", top_k=2, filters={\"year\": 2023})\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc2bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
