{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9717c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c777795",
   "metadata": {},
   "source": [
    "## Project 1: A Hybrid Vector Database with Reranking and Filtering\n",
    "\n",
    "This project is a great way to explore the full lifecycle of a RAG pipeline's retrieval component. A hybrid approach combines the speed of vector search with the precision of traditional keyword search and reranking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338f600",
   "metadata": {},
   "source": [
    "### Core Components:\n",
    "\n",
    "* Vector Indexing: Build an in-memory or on-disk vector index using a library like FAISS or ScaNN. Your database should be able to add, delete, and update vectors.\n",
    "\n",
    "* Metadata Storage: A relational database (like SQLite or PostgreSQL) or a simple JSON file store to hold the original text chunks and associated metadata (e.g., document ID, author, date, categories).\n",
    "\n",
    "* Hybrid Search:\n",
    "\n",
    "    * Vector Search: Use your vector index to find the top N most semantically similar documents to a user's query.\n",
    "\n",
    "    * Keyword Search: Implement a basic keyword search using a library like whoosh or even a simple inverted index to find documents with exact keyword matches.\n",
    "\n",
    "    * Combining Results: Fuse the results from both searches. You can use a simple scoring system that weighs both semantic similarity and keyword presence.\n",
    "\n",
    "* Reranking: The top results from the hybrid search may not be in the best order. Implement a reranker model (a small, performant cross-encoder from Hugging Face) that takes the user query and each of the top-retrieved documents and re-scores them to improve the final ranking.\n",
    "\n",
    "* Advanced Filtering: Allow users to filter their search results based on the metadata. For example, a user could search for \"deep learning\" but filter the results to only include documents from the \"2024\" year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb7f63",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "The goal of this project is to build a complete, end-to-end retrieval system that goes beyond a simple vector database. The system will be a \"hybrid\" search engine that combines three key retrieval methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cc417",
   "metadata": {},
   "source": [
    "* Vector Search: For semantic understanding and finding documents conceptually similar to a query. Documents and queries are converted into numerical vectors (embeddings). The system finds documents whose vectors are geometrically closest to the query vector.\n",
    "\n",
    "* Keyword Search: For precise matching of specific terms, which can be critical for proper names, acronyms, and direct references. A traditional search technique using an inverted index. It's excellent for finding exact term matches and is highly performant.\n",
    "\n",
    "* Reranking: To re-order the top-K results from the hybrid search, ensuring the most relevant documents are at the very top. A secondary, more powerful model (typically a cross-encoder) is used to re-evaluate the top results from the initial hybrid search. This is computationally more expensive, but since it only operates on a small number of candidates, it significantly improves the final accuracy.\n",
    "\n",
    "* Hybrid Search: The process of combining the results from both vector and keyword searches. The challenge here is how to score and merge the results to get a single, ranked list.\n",
    "\n",
    "* Metadata Filtering: The ability to narrow down search results based on specific attributes of the documents, such as author, date, or category. This adds a powerful layer of user control to the search process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f3cc3",
   "metadata": {},
   "source": [
    "### Proposed System Architecture\n",
    "\n",
    "The system will have two primary phases: an Ingestion Pipeline and a Query Pipeline.\n",
    "\n",
    "1. Ingestion Pipeline: This is where you prepare your knowledge base.\n",
    "\n",
    "Document Loader: A component that reads in raw documents (e.g., text files, PDFs).\n",
    "\n",
    "Text Splitter: Breaks down large documents into smaller, manageable chunks. This is crucial for RAG, as you want to retrieve specific relevant snippets, not entire long documents.\n",
    "\n",
    "Embedding Model: Converts each text chunk into a vector embedding.\n",
    "\n",
    "Metadata Extractor: Parses and extracts metadata (e.g., title, year, author, source) for each chunk.\n",
    "\n",
    "Storage: Stores the text chunks, their embeddings, and the associated metadata.\n",
    "\n",
    "2. Query Pipeline: This is how the system responds to a user's query.\n",
    "\n",
    "User Query: A user inputs a query and any desired filters.\n",
    "\n",
    "Query Embedding: The user's query is converted into an embedding using the same model as the ingestion pipeline.\n",
    "\n",
    "Parallel Search: The query is sent to two different search components simultaneously.\n",
    "\n",
    "Vector Database: Performs a semantic search to find the top-K semantically similar documents.\n",
    "\n",
    "Keyword Search Index: Performs a keyword search to find the top-K documents with matching terms.\n",
    "\n",
    "Hybrid Combination: The results from both searches are combined and a score is calculated for each document. A simple method is to re-rank the union of the two result sets.\n",
    "\n",
    "Metadata Filtering: The combined results are filtered to only include documents that match the user's specified metadata criteria.\n",
    "\n",
    "Reranking Model: The final top-N documents from the hybrid search are passed to a reranker model, which re-sorts them for optimal relevance.\n",
    "\n",
    "Final Results: The top documents are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f3e4",
   "metadata": {},
   "source": [
    "## Suggested Technology Stack\n",
    "\n",
    "Programming Language: Python is the standard for this type of project due to its rich ecosystem of libraries.\n",
    "\n",
    "Embedding Model: Use a pre-trained model from Hugging Face via the sentence-transformers library.\n",
    "\n",
    "Recommendation: BAAI/bge-small-en-v1.5 for strong performance.\n",
    "\n",
    "Vector Index: The FAISS library from Facebook AI is a highly optimized and performant library for similarity search.\n",
    "\n",
    "Keyword Index: The Whoosh library is a pure-Python library for creating and searching text. It's a great choice for this project as it's simple to set up and use.\n",
    "\n",
    "Reranking Model: Use a cross-encoder model from Hugging Face, also via the sentence-transformers library.\n",
    "\n",
    "Recommendation: cross-encoder/ms-marco-MiniLM-L-6-v2 is a good, lightweight choice.\n",
    "\n",
    "Data Storage: A simple JSON file or a lightweight database like TinyDB can be used to store the original text chunks and their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10b44e",
   "metadata": {},
   "source": [
    "## Project Workflow and Code Structure\n",
    "\n",
    "Your project can be organized into a few key modules:\n",
    "\n",
    "ingestion.py:\n",
    "\n",
    "A function to load a document.\n",
    "\n",
    "A function to split text into chunks.\n",
    "\n",
    "A function to generate embeddings using sentence-transformers.\n",
    "\n",
    "A function to build the FAISS index and the Whoosh index, and save the metadata to a file.\n",
    "\n",
    "database.py:\n",
    "\n",
    "A class that acts as the main interface to your hybrid database.\n",
    "\n",
    "Methods like add_documents(), delete_documents(), and hybrid_search().\n",
    "\n",
    "It will handle loading the FAISS index, the Whoosh index, and the metadata on startup.\n",
    "\n",
    "query.py:\n",
    "\n",
    "A simple command-line interface or a main function that demonstrates the query process.\n",
    "\n",
    "Takes user input for the query and filters.\n",
    "\n",
    "Calls the hybrid_search() method from your database class and prints the final reranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running, you will need to install the required libraries:\n",
    "# pip install sentence-transformers faiss-cpu whoosh\n",
    "# We will use faiss-cpu for simplicity, but for GPU support, you would install faiss-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers faiss-cpu whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b10cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from whoosh.filedb.filestore import FileStorage\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.qparser.default import MultifieldParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71219831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridDatabase:\n",
    "    \"\"\"\n",
    "    A class to manage the hybrid vector database, including loading indices,\n",
    "    performing searches, and reranking results.\n",
    "    \"\"\"\n",
    "    def __init__(self, index_dir=\"index\"):\n",
    "        \"\"\"\n",
    "        Initializes the database by loading the pre-built FAISS index, Whoosh index,\n",
    "        and metadata. Also loads the embedding and reranking models.\n",
    "        \"\"\"\n",
    "        print(\"Initializing HybridDatabase...\")\n",
    "        self.index_dir = index_dir\n",
    "        self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "        self.reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        self.faiss_index = None\n",
    "        self.whoosh_index = None\n",
    "        self.metadata = []\n",
    "\n",
    "        # Load the indices and metadata from the specified directory\n",
    "        self._load_indices()\n",
    "        print(\"Database initialized and ready to query.\")\n",
    "\n",
    "    def _load_indices(self):\n",
    "        \"\"\"\n",
    "        Loads the FAISS index, Whoosh index, and metadata from the index directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load FAISS index\n",
    "            self.faiss_index = faiss.read_index(os.path.join(self.index_dir, \"faiss.index\"))\n",
    "\n",
    "            # Load Whoosh index\n",
    "            storage = FileStorage(os.path.join(self.index_dir, \"whoosh\"))\n",
    "            self.whoosh_index = storage.open_index()\n",
    "\n",
    "            # Load metadata\n",
    "            with open(os.path.join(self.index_dir, \"metadata.json\"), 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading indices: {e}\")\n",
    "            print(\"Please run the ingestion pipeline first.\")\n",
    "            exit()\n",
    "\n",
    "    def hybrid_search(self, query: str, top_k: int = 10, alpha: float = 0.5, filters: dict = None):\n",
    "        \"\"\"\n",
    "        Performs a hybrid search combining vector search and keyword search.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's search query.\n",
    "            top_k (int): The number of top results to return.\n",
    "            alpha (float): A weighting factor for combining vector and keyword scores.\n",
    "            filters (dict): A dictionary of metadata filters (e.g., {\"year\": 2023}).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of reranked and filtered document snippets.\n",
    "        \"\"\"\n",
    "        # 1. Perform parallel searches\n",
    "        print(f\"\\nSearching for: '{query}'...\")\n",
    "\n",
    "        # Vector search (semantic search)\n",
    "        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        vector_distances, vector_ids = self.faiss_index.search(query_embedding, top_k * 2)\n",
    "\n",
    "        # Keyword search\n",
    "        keyword_results = []\n",
    "        with self.whoosh_index.searcher() as searcher:\n",
    "            query_parser = MultifieldParser([\"content\", \"title\"], schema=self.whoosh_index.schema)\n",
    "            parsed_query = query_parser.parse(query)\n",
    "            results = searcher.search(parsed_query, limit=top_k * 2)\n",
    "            keyword_results = [result['doc_id'] for result in results]\n",
    "\n",
    "        # 2. Combine and filter results\n",
    "        # Collect a union of unique document IDs from both search methods\n",
    "        unique_doc_ids = set(vector_ids[0]).union(set(keyword_results))\n",
    "        combined_candidates = []\n",
    "        for doc_id in unique_doc_ids:\n",
    "            doc_metadata = self.metadata[doc_id]\n",
    "            \n",
    "            # Apply metadata filters\n",
    "            if filters:\n",
    "                is_filtered = False\n",
    "                for key, value in filters.items():\n",
    "                    if key in doc_metadata and doc_metadata[key] != value:\n",
    "                        is_filtered = True\n",
    "                        break\n",
    "                if is_filtered:\n",
    "                    continue\n",
    "\n",
    "            combined_candidates.append(doc_metadata['content'])\n",
    "        \n",
    "        # If no results after filtering, return empty list\n",
    "        if not combined_candidates:\n",
    "            print(\"No results found after filtering.\")\n",
    "            return []\n",
    "\n",
    "        # 3. Rerank the top candidates\n",
    "        # Create a list of tuples for the reranker model: [(query, doc1), (query, doc2), ...]\n",
    "        cross_encoder_input = [(query, doc) for doc in combined_candidates]\n",
    "        reranker_scores = self.reranker_model.predict(cross_encoder_input)\n",
    "\n",
    "        # 4. Sort the candidates based on reranker scores\n",
    "        scored_results = sorted(zip(combined_candidates, reranker_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top_k results\n",
    "        return [result[0] for result in scored_results[:top_k]]\n",
    "\n",
    "\n",
    "def ingest_documents(documents: list, index_dir=\"index\"):\n",
    "    \"\"\"\n",
    "    Ingests documents into the hybrid database. This function simulates\n",
    "    the ingestion pipeline, creating and saving the indices and metadata.\n",
    "    \"\"\"\n",
    "    print(\"Starting document ingestion pipeline...\")\n",
    "    # Create index directories\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "    if not os.path.exists(os.path.join(index_dir, \"whoosh\")):\n",
    "        os.makedirs(os.path.join(index_dir, \"whoosh\"))\n",
    "\n",
    "    # Load the embedding model\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "    # Prepare document chunks and metadata\n",
    "    metadata = []\n",
    "    chunk_id = 0\n",
    "    # For a real-world app, you would use a proper text splitter here.\n",
    "    # We will use simple splitting for this example.\n",
    "    for doc in documents:\n",
    "        # Split documents into smaller chunks\n",
    "        chunks = doc['content'].split('. ')\n",
    "        for chunk in chunks:\n",
    "            if chunk:\n",
    "                # Add a unique ID and other metadata\n",
    "                chunk_metadata = {\n",
    "                    \"doc_id\": chunk_id,\n",
    "                    \"title\": doc.get('title', 'Unknown'),\n",
    "                    \"author\": doc.get('author', 'Unknown'),\n",
    "                    \"year\": doc.get('year', 'Unknown'),\n",
    "                    \"content\": chunk.strip() + '.'\n",
    "                }\n",
    "                metadata.append(chunk_metadata)\n",
    "                chunk_id += 1\n",
    "\n",
    "    # 1. Build FAISS index for vector search\n",
    "    print(\"Building FAISS vector index...\")\n",
    "    text_chunks = [item['content'] for item in metadata]\n",
    "    embeddings = embedding_model.encode(text_chunks, convert_to_numpy=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    faiss_index.add(embeddings)\n",
    "    faiss.write_index(faiss_index, os.path.join(index_dir, \"faiss.index\"))\n",
    "    print(\"FAISS index built and saved.\")\n",
    "\n",
    "    # 2. Build Whoosh index for keyword search\n",
    "    print(\"Building Whoosh keyword index...\")\n",
    "    schema = Schema(doc_id=ID(stored=True), title=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "                    author=TEXT(stored=True), year=NUMERIC(stored=True),\n",
    "                    content=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
    "    storage = FileStorage(os.path.join(index_dir, \"whoosh\"))\n",
    "    whoosh_ix = create_in(storage, schema)\n",
    "    writer = whoosh_ix.writer()\n",
    "    for item in metadata:\n",
    "        writer.add_document(doc_id=str(item['doc_id']), title=item['title'], author=item['author'],\n",
    "                            year=item['year'], content=item['content'])\n",
    "    writer.commit()\n",
    "    print(\"Whoosh index built and saved.\")\n",
    "\n",
    "    # 3. Save metadata\n",
    "    print(\"Saving metadata...\")\n",
    "    with open(os.path.join(index_dir, \"metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(\"Metadata saved.\")\n",
    "    print(\"Ingestion pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Mock data to simulate new documents being added to the knowledge base\n",
    "    mock_documents = [\n",
    "        {\n",
    "            \"title\": \"The Rise of AI\",\n",
    "            \"author\": \"Alice\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Artificial intelligence has seen a massive surge in popularity. \"\n",
    "                \"Large Language Models, or LLMs, are at the forefront of this revolution. \"\n",
    "                \"These models use a transformer architecture to process and generate human-like text. \"\n",
    "                \"The core technology of LLMs is based on deep learning principles.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Deep Learning Techniques\",\n",
    "            \"author\": \"Bob\",\n",
    "            \"year\": 2023,\n",
    "            \"content\": (\n",
    "                \"Deep learning is a subset of machine learning that utilizes neural networks. \"\n",
    "                \"A key component is the backpropagation algorithm. \"\n",
    "                \"Transfer learning is a technique that uses pre-trained models. \"\n",
    "                \"Researchers are exploring new ways to optimize these complex models.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Vector Databases Explained\",\n",
    "            \"author\": \"Alice\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Vector databases are specialized databases for storing and searching vector embeddings. \"\n",
    "                \"They are crucial for efficient semantic search in RAG applications. \"\n",
    "                \"FAISS and Milvus are popular examples of vector database technologies. \"\n",
    "                \"These databases use approximate nearest neighbor algorithms.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Quantum Computing\",\n",
    "            \"author\": \"Charlie\",\n",
    "            \"year\": 2024,\n",
    "            \"content\": (\n",
    "                \"Quantum computing promises to solve problems intractable for classical computers. \"\n",
    "                \"It uses principles of quantum mechanics, such as superposition and entanglement. \"\n",
    "                \"Qubits are the fundamental units of quantum information. \"\n",
    "                \"The field is still in its early stages of development.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Step 1: Ingestion ---\n",
    "    # This step would typically be run periodically to update the knowledge base.\n",
    "    # It creates all the necessary indices and stores the metadata.\n",
    "    ingest_documents(mock_documents)\n",
    "\n",
    "    # --- Step 2: Querying ---\n",
    "    # This simulates a user querying the already-ingested knowledge base.\n",
    "    db = HybridDatabase()\n",
    "\n",
    "    # Example 1: Pure semantic search query\n",
    "    print(\"--- Example 1: Semantic Query ---\")\n",
    "    results = db.hybrid_search(query=\"How do LLMs work?\", top_k=3)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n",
    "\n",
    "    # Example 2: Keyword-heavy search query\n",
    "    print(\"\\n--- Example 2: Keyword Query ---\")\n",
    "    results = db.hybrid_search(query=\"superposition and entanglement\", top_k=2)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n",
    "\n",
    "    # Example 3: Hybrid query with filtering\n",
    "    print(\"\\n--- Example 3: Hybrid Query with Metadata Filter ---\")\n",
    "    # This query looks for information about AI, but only from documents published in 2023\n",
    "    results = db.hybrid_search(query=\"what is transfer learning in AI?\", top_k=2, filters={\"year\": 2023})\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. {res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557f7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311405de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
